{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jayanths9/Dodelido_opencv/blob/main/Jay_Single_Image_Train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2l8hbH30cFe"
      },
      "outputs": [],
      "source": [
        "# Importing Library Files\n",
        "import albumentations as A\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import json\n",
        "#from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_V5hYh-YEeL"
      },
      "outputs": [],
      "source": [
        "image_dir=\"Images/\"\n",
        "data = pd.read_json(\"labels.json\").T\n",
        "\n",
        "data = data.sort_index(ascending=True)\n",
        "data = data.reset_index()\n",
        "\n",
        "data[\"animal_color\"] = data.apply(lambda row: [row[0], row[1]], axis=1)\n",
        "data['labels'] = list(zip(data[0], data[1]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iaraqYZbqJf",
        "outputId": "1160b612-3c4a-4a6d-8a76-35f7a8d90d3d"
      },
      "outputs": [],
      "source": [
        "# Splitting the dataset\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    data['index'], data['animal_color'], test_size=0.2, random_state=44)\n",
        "\n",
        "print(\"Number of posters for training: \", len(X_train))\n",
        "print(\"Number of posters for validation: \", len(X_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ht_8kbnKrtXT",
        "outputId": "c66dfd14-23b5-4598-ff08-7cdf8777b755"
      },
      "outputs": [],
      "source": [
        "# The targets should be a list of list of strings to fit a binarizer (multi-hot encoding).\n",
        "\n",
        "y_train = list(y_train)\n",
        "y_val = list(y_val)\n",
        "\n",
        "print(y_val[:4])\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "# Fit the multi-label binarizer on the training set\n",
        "mlb = MultiLabelBinarizer()\n",
        "mlb.fit(y_train)\n",
        "\n",
        "# Loop over all labels and show them\n",
        "N_LABELS = len(mlb.classes_)\n",
        "for (i, label) in enumerate(mlb.classes_):\n",
        "    print(\"{}. {}\".format(i, label))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlaHYnkX4hAT",
        "outputId": "7334135b-97d9-4de1-edf8-82aff48bb6b8"
      },
      "outputs": [],
      "source": [
        "# transform the targets of the training and test sets\n",
        "y_train_bin = mlb.transform(y_train)\n",
        "y_val_bin = mlb.transform(y_val)\n",
        "\n",
        "# Print example of images and their binary targets\n",
        "for i in range(3):\n",
        "    print(X_train[i], y_train_bin[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8rE5G7HPICr"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "IMG_SIZE = 224 # Specify height and width of image to match the input format of the model\n",
        "CHANNELS = 3 # Keep RGB color channels to match the input format of the model\n",
        "BATCH_SIZE = 256 # Big enough to measure an F1-score\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE # Adapt preprocessing and prefetching dynamically\n",
        "SHUFFLE_BUFFER_SIZE = 1024 # Shuffle the training data by a chunck of 1024 observations\n",
        "\n",
        "def parse_function(filename, label):\n",
        "    \"\"\"Function that returns a tuple of normalized image array and labels array.\n",
        "    Args:\n",
        "        filename: string representing path to image\n",
        "        label: 0/1 one-dimensional array of size N_LABELS\n",
        "    \"\"\"\n",
        "    # filename=image_dir+filename\n",
        "    # Read an image from a file\n",
        "    image_string = tf.io.read_file(filename)\n",
        "    # Decode it into a dense vector\n",
        "    image_decoded = tf.image.decode_jpeg(image_string, channels=CHANNELS)\n",
        "    # Resize it to fixed shape\n",
        "    image_resized = tf.image.resize(image_decoded, [IMG_SIZE, IMG_SIZE])\n",
        "    # Normalize it from [0, 255] to [0.0, 1.0]\n",
        "    image_normalized = image_resized / 255.0\n",
        "    return image_normalized, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_dataset(filenames, labels, is_training=True):\n",
        "    \"\"\"Load and parse dataset.\n",
        "    Args:\n",
        "        filenames: list of image paths\n",
        "        labels: numpy array of shape (BATCH_SIZE, N_LABELS)\n",
        "        is_training: boolean to indicate training mode\n",
        "    \"\"\"\n",
        "    filenames=image_dir+filenames\n",
        "    \n",
        "\n",
        "    # Create a first dataset of file paths and labels\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
        "    # Parse and preprocess observations in parallel\n",
        "    dataset = dataset.map(parse_function, num_parallel_calls=AUTOTUNE)\n",
        "    dataset = dataset.map(lambda x, y: (x, y, print(x[0], flush=True)))\n",
        "\n",
        "        \n",
        "\n",
        "    if is_training == True:\n",
        "        # This is a small dataset, only load it once, and keep it in memory.\n",
        "        dataset = dataset.cache()\n",
        "        # Shuffle the data each buffer size\n",
        "        dataset = dataset.shuffle(buffer_size=SHUFFLE_BUFFER_SIZE)\n",
        "\n",
        "    # Batch the data for multiple steps\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    # Fetch batches in the background while the model is training.\n",
        "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRysljQuRE1-"
      },
      "outputs": [],
      "source": [
        "train_ds1 = create_dataset(X_train, y_train_bin)\n",
        "val_ds1 = create_dataset(X_val, y_val_bin)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcNo0XR2RVn8"
      },
      "source": [
        "\n",
        "\n",
        "Each batch will be a pair of arrays (one that holds the features and another one that holds the labels).\n",
        "The features array will be of shape (BATCH_SIZE, IMG_SIZE, IMG_SIZE, CHANNELS).\n",
        "The labels array will be of shape (BATCH_SIZE, N_LABELS) where N_LABELS is the maximum number of labels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGaCiGaQRghv"
      },
      "outputs": [],
      "source": [
        "import tensorflow_hub as hub\n",
        "from keras import layers\n",
        "\n",
        "feature_extractor_url = \"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4\"\n",
        "feature_extractor_layer = hub.KerasLayer(feature_extractor_url,\n",
        "                                         input_shape=(IMG_SIZE,IMG_SIZE,CHANNELS))\n",
        "feature_extractor_layer.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgUZaNnWTp-l",
        "outputId": "9d7a5728-2418-451b-904a-ffef428d954f"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential(\n",
        "    [feature_extractor_layer,\n",
        "    layers.Dense(1024, activation='relu', name='hidden_layer'),\n",
        "    layers.Dense(N_LABELS, activation='sigmoid', name='output')])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mF6ZitpKEXL5"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def macro_soft_f1(y, y_hat):\n",
        "    \"\"\"Compute the macro soft F1-score as a cost (average 1 - soft-F1 across all labels).\n",
        "    Use probability values instead of binary predictions.\n",
        "\n",
        "    Args:\n",
        "        y (int32 Tensor): targets array of shape (BATCH_SIZE, N_LABELS)\n",
        "        y_hat (float32 Tensor): probability matrix from forward propagation of shape (BATCH_SIZE, N_LABELS)\n",
        "\n",
        "    Returns:\n",
        "        cost (scalar Tensor): value of the cost function for the batch\n",
        "    \"\"\"\n",
        "    y = tf.cast(y, tf.float32)\n",
        "    y_hat = tf.cast(y_hat, tf.float32)\n",
        "    tp = tf.reduce_sum(y_hat * y, axis=0)\n",
        "    fp = tf.reduce_sum(y_hat * (1 - y), axis=0)\n",
        "    fn = tf.reduce_sum((1 - y_hat) * y, axis=0)\n",
        "    soft_f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
        "    cost = 1 - soft_f1 # reduce 1 - soft-f1 in order to increase soft-f1\n",
        "    macro_cost = tf.reduce_mean(cost) # average on all labels\n",
        "    return macro_cost\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def macro_f1(y, y_hat, thresh=0.5):\n",
        "    \"\"\"Compute the macro F1-score on a batch of observations (average F1 across labels)\n",
        "\n",
        "    Args:\n",
        "        y (int32 Tensor): labels array of shape (BATCH_SIZE, N_LABELS)\n",
        "        y_hat (float32 Tensor): probability matrix from forward propagation of shape (BATCH_SIZE, N_LABELS)\n",
        "        thresh: probability value above which we predict positive\n",
        "\n",
        "    Returns:\n",
        "        macro_f1 (scalar Tensor): value of macro F1 for the batch\n",
        "    \"\"\"\n",
        "    y_pred = tf.cast(tf.greater(y_hat, thresh), tf.float32)\n",
        "    tp = tf.cast(tf.math.count_nonzero(y_pred * y, axis=0), tf.float32)\n",
        "    fp = tf.cast(tf.math.count_nonzero(y_pred * (1 - y), axis=0), tf.float32)\n",
        "    fn = tf.cast(tf.math.count_nonzero((1 - y_pred) * y, axis=0), tf.float32)\n",
        "    f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
        "    macro_f1 = tf.reduce_mean(f1)\n",
        "    return macro_f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z79DO-MqEq__",
        "outputId": "e81dc397-d486-4832-b555-0867506ae3b2"
      },
      "outputs": [],
      "source": [
        "LR = 1e-4 # Keep it small when transfer learning\n",
        "EPOCHS = 100\n",
        "PATIENCE=5\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stop = EarlyStopping(monitor = 'val_loss', patience = PATIENCE)\n",
        "model.compile(\n",
        "  optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
        "  loss=macro_soft_f1,\n",
        "  metrics=[macro_f1])\n",
        "# Train the model\n",
        "history = model.fit(train_ds1,\n",
        "                    epochs=EPOCHS,\n",
        "                    validation_data=create_dataset(X_val, y_val_bin,\n",
        "                                                   is_training=False),callbacks = [early_stop])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check Keras version (should be 2.10.x or higher)\n",
        "from tensorflow.keras import __version__\n",
        "print(f\"Keras version: {__version__}\")\n",
        "\n",
        "# Define your custom metrics (macro_soft_f1 and macro_f1)\n",
        "\n",
        "# Assuming you have a model trained with these custom metrics\n",
        "tf.keras.models.save_model('dodeLidoModel', custom_objects={'macro_soft_f1': macro_soft_f1, 'macro_f1': macro_f1})"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.19 ('py38')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    },
    "vscode": {
      "interpreter": {
        "hash": "2e0ac825c0b43c8fdb3b41fbc96e439030dce5eafdf638a4a14de2f62742eef0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
