{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import load_model\n",
    "from tkinter import *\n",
    "from PIL import Image, ImageTk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_soft_f1(y, y_hat):\n",
    "    \"\"\"Compute the macro soft F1-score as a cost (average 1 - soft-F1 across all labels).\n",
    "    Use probability values instead of binary predictions.\n",
    "\n",
    "    Args:\n",
    "        y (int32 Tensor): targets array of shape (BATCH_SIZE, N_LABELS)\n",
    "        y_hat (float32 Tensor): probability matrix from forward propagation of shape (BATCH_SIZE, N_LABELS)\n",
    "\n",
    "    Returns:\n",
    "        cost (scalar Tensor): value of the cost function for the batch\n",
    "    \"\"\"\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    y_hat = tf.cast(y_hat, tf.float32)\n",
    "    tp = tf.reduce_sum(y_hat * y, axis=0)\n",
    "    fp = tf.reduce_sum(y_hat * (1 - y), axis=0)\n",
    "    fn = tf.reduce_sum((1 - y_hat) * y, axis=0)\n",
    "    soft_f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
    "    cost = 1 - soft_f1 # reduce 1 - soft-f1 in order to increase soft-f1\n",
    "    macro_cost = tf.reduce_mean(cost) # average on all labels\n",
    "    return macro_cost\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def macro_f1(y, y_hat, thresh=0.5):\n",
    "    \"\"\"Compute the macro F1-score on a batch of observations (average F1 across labels)\n",
    "\n",
    "    Args:\n",
    "        y (int32 Tensor): labels array of shape (BATCH_SIZE, N_LABELS)\n",
    "        y_hat (float32 Tensor): probability matrix from forward propagation of shape (BATCH_SIZE, N_LABELS)\n",
    "        thresh: probability value above which we predict positive\n",
    "\n",
    "    Returns:\n",
    "        macro_f1 (scalar Tensor): value of macro F1 for the batch\n",
    "    \"\"\"\n",
    "    y_pred = tf.cast(tf.greater(y_hat, thresh), tf.float32)\n",
    "    tp = tf.cast(tf.math.count_nonzero(y_pred * y, axis=0), tf.float32)\n",
    "    fp = tf.cast(tf.math.count_nonzero(y_pred * (1 - y), axis=0), tf.float32)\n",
    "    fn = tf.cast(tf.math.count_nonzero((1 - y_pred) * y, axis=0), tf.float32)\n",
    "    f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
    "    macro_f1 = tf.reduce_mean(f1)\n",
    "    return macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the pre-trained model\n",
    "model = load_model('dodeLidoModel', custom_objects={'macro_soft_f1': macro_soft_f1, 'macro_f1': macro_f1})\n",
    "\n",
    "# Define class names\n",
    "class_names = [\"Alarm\", \"Blue\", \"Flamingo\", \"Giraffe\", \"Green\", \"Grey\", \"Lion\", \"Monkey\", \"Pink\", \"Sloth\", \"Snake\", \"Yellow\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=val_ds1\n",
    "images=X[0]\n",
    "labels=np.array(X[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'pop from an empty set'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m predictions\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      2\u001b[0m [predictions[\u001b[38;5;241m0\u001b[39m]]\n",
      "File \u001b[1;32mc:\\Users\\akhil\\anaconda3\\envs\\py38\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\akhil\\anaconda3\\envs\\py38\\lib\\site-packages\\keras\\engine\\data_adapter.py:259\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    253\u001b[0m (sample_weights, _, _) \u001b[39m=\u001b[39m training_utils\u001b[39m.\u001b[39mhandle_partial_sample_weights(\n\u001b[0;32m    254\u001b[0m     y, sample_weights, sample_weight_modes, check_all_flat\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    255\u001b[0m )\n\u001b[0;32m    257\u001b[0m inputs \u001b[39m=\u001b[39m pack_x_y_sample_weight(x, y, sample_weights)\n\u001b[1;32m--> 259\u001b[0m num_samples \u001b[39m=\u001b[39m \u001b[39mset\u001b[39;49m(\n\u001b[0;32m    260\u001b[0m     \u001b[39mint\u001b[39;49m(i\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m]) \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m tf\u001b[39m.\u001b[39;49mnest\u001b[39m.\u001b[39;49mflatten(inputs)\n\u001b[0;32m    261\u001b[0m )\u001b[39m.\u001b[39;49mpop()\n\u001b[0;32m    262\u001b[0m _check_data_cardinality(inputs)\n\u001b[0;32m    264\u001b[0m \u001b[39m# If batch_size is not passed but steps is, calculate from the input\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[39m# data.  Default to 32 for backwards compat.\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'pop from an empty set'"
     ]
    }
   ],
   "source": [
    "predictions=np.array(model.predict(images))\n",
    "[predictions[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Convert values to binary array based on the threshold\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m binary_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(np\u001b[38;5;241m.\u001b[39marray(\u001b[43mpredictions\u001b[49m) \u001b[38;5;241m>\u001b[39m threshold, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the threshold\n",
    "threshold = 0.5\n",
    "\n",
    "# Convert values to binary array based on the threshold\n",
    "binary_array = np.where(np.array(predictions) > threshold, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(mlb.classes_)  # Get the number of classes from mlb\n",
    "\n",
    "#actual_labels = tf.one_hot(labels, num_classes)  # One-hot encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels=mlb.inverse_transform(binary_array)\n",
    "actual_labels=mlb.inverse_transform(labels)\n",
    "\n",
    "for i in range(len(predicted_labels)):\n",
    "  print(actual_labels[i],predicted_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "def read_image(image_object):\n",
    "  image_resized = tf.image.resize(image_object,[IMG_SIZE, IMG_SIZE])\n",
    "  # Normalize it from [0, 255] to [0.0, 1.0]\n",
    "  image_normalized = image_resized / 255.0\n",
    "  expanded_tensor = tf.expand_dims(image_normalized, axis=0)\n",
    "  ouptut=model.predict(expanded_tensor)\n",
    "  result_list = list(ouptut)\n",
    "  binary_array = np.where(np.array(result_list) > threshold, 1, 0)\n",
    "  result=mlb.inverse_transform(binary_array)\n",
    "  return result\n",
    "\n",
    "\n",
    "def filter_overlapping_circles(circles, min_dist_between_circles):\n",
    "  filtered_circles = []\n",
    "\n",
    "  for circle in circles[0]:\n",
    "      x, y, r = circle\n",
    "      overlap = False\n",
    "      for fc in filtered_circles:\n",
    "          fx, fy, fr = fc\n",
    "          distance = np.sqrt((x - fx) ** 2 + (y - fy) ** 2)\n",
    "          if distance < min_dist_between_circles:\n",
    "              overlap = True\n",
    "              break\n",
    "      if not overlap:\n",
    "          filtered_circles.append(circle)\n",
    "\n",
    "  return (filtered_circles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 10s 10s/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mlb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m y2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], y \u001b[38;5;241m+\u001b[39m r\u001b[38;5;241m+\u001b[39mbuffer)\n\u001b[0;32m     42\u001b[0m new_image\u001b[38;5;241m=\u001b[39m image[y1:y2, x1:x2]\n\u001b[1;32m---> 43\u001b[0m temp\u001b[38;5;241m=\u001b[39m\u001b[43mread_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2RGB\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(temp)\n\u001b[0;32m     45\u001b[0m cv2\u001b[38;5;241m.\u001b[39mrectangle(image, (x1, y1), (x2, y2), (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[1;32mIn[26], line 10\u001b[0m, in \u001b[0;36mread_image\u001b[1;34m(image_object)\u001b[0m\n\u001b[0;32m      8\u001b[0m result_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ouptut)\n\u001b[0;32m      9\u001b[0m binary_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(np\u001b[38;5;241m.\u001b[39marray(result_list) \u001b[38;5;241m>\u001b[39m threshold, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m result\u001b[38;5;241m=\u001b[39m\u001b[43mmlb\u001b[49m\u001b[38;5;241m.\u001b[39minverse_transform(binary_array)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mlb' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "path=\"IMG_2018.JPG\"\n",
    "target_width = 1024\n",
    "target_height = 1024\n",
    "image=cv2.imread(path)\n",
    "# image = cv2.resize(cv2_image, (target_width, target_height))\n",
    "\n",
    "\n",
    "# Convert the image to grayscale\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "# Use Hough Circle Transform to detect circles\n",
    "blurred = cv2.GaussianBlur(gray, (11, 11), 0)\n",
    "circles = cv2.HoughCircles(\n",
    "    blurred,\n",
    "    cv2.HOUGH_GRADIENT,\n",
    "    dp=1,\n",
    "    minDist=500,         # Adjust based on expected circle density\n",
    "    param1=100,         # Higher threshold for Canny edge detector\n",
    "    param2=30,          # Lower accumulator threshold for circle detection\n",
    "    minRadius=100,       # Adjust based on the smallest expected circle size\n",
    "    maxRadius=450       # Adjust based on the largest expected circle size\n",
    ")\n",
    "\n",
    "images=[]\n",
    "buffer=20\n",
    "if circles is not None:\n",
    "    # Convert the (x, y) coordinates and radius of the circles to integers\n",
    "    circles = circles.astype(int)\n",
    "    min_dist_between_circles = 500  # Adjust this value as needed\n",
    "    filtered_circles = filter_overlapping_circles(circles, min_dist_between_circles)\n",
    "\n",
    "    # Loop over all detected circles\n",
    "    for circle in filtered_circles:\n",
    "        # Extract the coordinates and radius of the circle\n",
    "        x, y, r = circle\n",
    "\n",
    "        # Draw the bounding box around the circle\n",
    "        x1 = max(0, x - r-buffer)\n",
    "        y1 = max(0, y - r-buffer)\n",
    "        x2 = min(image.shape[1], x + r+buffer)\n",
    "        y2 = min(image.shape[0], y + r+buffer)\n",
    "\n",
    "        new_image= image[y1:y2, x1:x2]\n",
    "        temp=read_image(cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB))\n",
    "        print(temp)\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        text_to_display = temp[0][0] + \", \" + temp[0][1]\n",
    "        cv2.putText(image, text_to_display, (x1, y1- 20), cv2.FONT_HERSHEY_SIMPLEX, 3, (255, 0, 0), 2)\n",
    "\n",
    "        images.append(image)\n",
    "        # Draw the bounding box on the original image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.19 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2e0ac825c0b43c8fdb3b41fbc96e439030dce5eafdf638a4a14de2f62742eef0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
